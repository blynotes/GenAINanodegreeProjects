# Hugging Face PEFT Library
## Key Concepts
Hugging Face PEFT allows you to fine-tune a model without having to fine-tune all of its parameters.

Training a model using Hugging Face PEFT requires two additional steps beyond traditional fine-tuning:

1. Creating a __PEFT config__
2. __Converting the model into a PEFT model__ using the PEFT config
Inference using a PEFT model is almost identical to inference using a non-PEFT model. The only difference is that it must be __loaded as a PEFT model__.

## Training with PEFT
### Creating a PEFT Config
The PEFT config specifies the adapter configuration for your parameter-efficient fine-tuning process. The base class for this is a `PeftConfig`, but this example will use a `LoraConfig`, the subclass used for low rank adaptation (LoRA).

A LoRA config can be instantiated like this:
```python
from peft import LoraConfig
config = LoraConfig()
```
Look at the LoRA adapter documentation for additional hyperparameters that can be specified by passing arguments to `LoraConfig()`. The [Hugging Face LoRA conceptual guide](https://huggingface.co/docs/peft/conceptual_guides/lora) also contains additional explanations.

### Converting a Transformers Model into a PEFT Model
Once you have a PEFT config object, you can load a Hugging Face `transformers` model as a PEFT model by first loading the pre-trained model as usual (here we load GPT-2):

```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("gpt2")
```
Then `using get_peft_model()` to get a trainable PEFT model (using the LoRA config instantiated previously):

```python
from peft import get_peft_model
lora_model = get_peft_model(model, config)
```

### Training with a PEFT Model
After calling `get_peft_model()`, you can then use the resulting `lora_model` in a training process of your choice (PyTorch training loop or Hugging Face `Trainer`).

### Checking Trainable Parameters of a PEFT Model
A helpful way to check the number of trainable parameters with the current config is the `print_trainable_parameters()` method:

```python
lora_model.print_trainable_parameters()
```
Which prints an output like this:
```
trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.23643136409814364
```

### Saving a Trained PEFT Model
Once a PEFT model has been trained, the standard Hugging Face `save_pretrained()` method can be used to save the weights locally. For example:
```python
lora_model.save_pretrained("gpt-lora")
```
Note that this __only saves the adapter weights__ and not the weights of the original Transformers model. Thus the size of the files created will be much smaller than you might expect.

## Inference with PEFT
### Loading a Saved PEFT Model
Because you have only saved the adapter weights and not the full model weights, you can't use `from_pretrained()` with the regular Transformers class (e.g., `AutoModelForCausalLM`). Instead, you need to use the PEFT version (e.g., `AutoPeftModelForCausalLM`). For example:
```python
from peft import AutoPeftModelForCausalLM
lora_model = AutoPeftModelForCausalLM.from_pretrained("gpt-lora")
```
After completing this step, you can proceed to use the model for inference.

### Generating Text from a PEFT Model
You may see examples from regular Transformer models where the input IDs are passed in as a positional argument (e.g., `model.generate(input_ids)`). For a PEFT model, they must be passed in as a keyword argument (e.g., `model.generate(input_ids=input_ids)`). For example:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
inputs = tokenizer("Hello, my name is ", return_tensors="pt")
outputs = model.generate(input_ids=inputs["input_ids"], max_new_tokens=10)
print(tokenizer.batch_decode(outputs))
```

## Documentation Links
* [Hugging Face PEFT configuration](https://huggingface.co/docs/peft/package_reference/config)
* [Hugging Face LoRA adapter](https://huggingface.co/docs/peft/package_reference/lora)
* [Hugging Face Models save_pretrained](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
* [Hugging Face Text Generation](https://huggingface.co/docs/transformers/main_classes/text_generation)